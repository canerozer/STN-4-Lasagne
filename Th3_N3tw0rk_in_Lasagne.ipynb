{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 780 (CNMeM is disabled, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['THEANO_FLAGS']='device=gpu0'\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "import matplotlib.pyplot as plt\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "conv = lasagne.layers.Conv2DLayer\n",
    "pool = lasagne.layers.MaxPool2DLayer\n",
    "\n",
    "from lasagne.utils import as_tuple, floatX\n",
    "from lasagne.random import get_rng\n",
    "import cv2\n",
    "\n",
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 192\n",
    "LEARNING_RATE = 0.001\n",
    "DIM = 60\n",
    "NUM_CLASSES = 10\n",
    "mnist_cluttered = \"mnist_cluttered_60x60_6distortions.npz\"\n",
    "#INPUT FILE DEGISTIRILECEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-02-28 12:47:48--  https://s3.amazonaws.com/lasagne/recipes/datasets/mnist_cluttered_60x60_6distortions.npz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.16.123\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.16.123|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 43046126 (41M) [application/octet-stream]\n",
      "Server file no newer than local file ‘mnist_cluttered_60x60_6distortions.npz’ -- not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -N https://s3.amazonaws.com/lasagne/recipes/datasets/mnist_cluttered_60x60_6distortions.npz\n",
    "#INPUT FILE DEGISTIRILECEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TransformerLayer(lasagne.layers.MergeLayer):\n",
    "    \"\"\"\n",
    "    Spatial transformer layer\n",
    "    The layer applies an affine transformation on the input. The affine\n",
    "    transformation is parameterized with six learned parameters [1]_.\n",
    "    The output is interpolated with a bilinear transformation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    incoming : a :class:`Layer` instance or a tuple\n",
    "        The layer feeding into this layer, or the expected input shape. The\n",
    "        output of this layer should be a 4D tensor, with shape\n",
    "        ``(batch_size, num_input_channels, input_rows, input_columns)``.\n",
    "    localization_network : a :class:`Layer` instance\n",
    "        The network that calculates the parameters of the affine\n",
    "        transformation. See the example for how to initialize to the identity\n",
    "        transform.\n",
    "    downsample_factor : float or iterable of float\n",
    "        A float or a 2-element tuple specifying the downsample factor for the\n",
    "        output image (in both spatial dimensions). A value of 1 will keep the\n",
    "        original size of the input. Values larger than 1 will downsample the\n",
    "        input. Values below 1 will upsample the input.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1]  Max Jaderberg, Karen Simonyan, Andrew Zisserman,\n",
    "            Koray Kavukcuoglu (2015):\n",
    "            Spatial Transformer Networks. NIPS 2015,\n",
    "            http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf\n",
    "    Examples\n",
    "    --------\n",
    "    Here we set up the layer to initially do the identity transform, similarly\n",
    "    to [1]_. Note that you will want to use a localization with linear output.\n",
    "    If the output from the localization networks is [t1, t2, t3, t4, t5, t6]\n",
    "    then t1 and t5 determines zoom, t2 and t4 determines skewness, and t3 and\n",
    "    t6 move the center position.\n",
    "    >>> import numpy as np\n",
    "    >>> import lasagne\n",
    "    >>> b = np.zeros((2, 3), dtype='float32')\n",
    "    >>> b[0, 0] = 1\n",
    "    >>> b[1, 1] = 1\n",
    "    >>> b = b.flatten()  # identity transform\n",
    "    >>> W = lasagne.init.Constant(0.0)\n",
    "    >>> l_in = lasagne.layers.InputLayer((None, 3, 28, 28))\n",
    "    >>> l_loc = lasagne.layers.DenseLayer(l_in, num_units=6, W=W, b=b,\n",
    "    ... nonlinearity=None)\n",
    "    >>> l_trans = lasagne.layers.TransformerLayer(l_in, l_loc)\n",
    "    \"\"\"\n",
    "    def __init__(self, incoming, localization_network, downsample_factor=1,\n",
    "                 **kwargs):\n",
    "        super(TransformerLayer, self).__init__(\n",
    "            [incoming, localization_network], **kwargs)\n",
    "        self.downsample_factor = as_tuple(downsample_factor, 2)\n",
    "\n",
    "        #self.y has been added.\n",
    "        input_shp, loc_shp = self.input_shapes\n",
    "\n",
    "        if loc_shp[-1] != 6 or len(loc_shp) != 2:\n",
    "            raise ValueError(\"The localization network must have \"\n",
    "                             \"output shape: (batch_size, 6)\")\n",
    "        if len(input_shp) != 4:\n",
    "            raise ValueError(\"The input network must have a 4-dimensional \"\n",
    "                             \"output shape: (batch_size, num_input_channels, \"\n",
    "                             \"input_rows, input_columns)\")\n",
    "\n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        shape = input_shapes[0]\n",
    "        factors = self.downsample_factor\n",
    "        return (shape[:2] + tuple(None if s is None else int(s // f)\n",
    "                                  for s, f in zip(shape[2:], factors)))\n",
    "\n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        # see eq. (1) and sec 3.1 in [1]\n",
    "        input, theta = inputs\n",
    "        self.x, self.y = _transform_affine(theta, input, self.downsample_factor)\n",
    "        return self.x\n",
    "    \n",
    "def _meshgrid(height, width):\n",
    "    # This function is the grid generator from eq. (1) in reference [1].\n",
    "    # It is equivalent to the following numpy code:\n",
    "    #  x_t, y_t = np.meshgrid(np.linspace(-1, 1, width),\n",
    "    #                         np.linspace(-1, 1, height))\n",
    "    #  ones = np.ones(np.prod(x_t.shape))\n",
    "    #  grid = np.vstack([x_t.flatten(), y_t.flatten(), ones])\n",
    "    # It is implemented in Theano instead to support symbolic grid sizes.\n",
    "    # Note: If the image size is known at layer construction time, we could\n",
    "    # compute the meshgrid offline in numpy instead of doing it dynamically\n",
    "    # in Theano. However, it hardly affected performance when we tried.\n",
    "    x_t = T.dot(T.ones((height, 1)),\n",
    "                _linspace(-1.0, 1.0, width).dimshuffle('x', 0))\n",
    "    y_t = T.dot(_linspace(-1.0, 1.0, height).dimshuffle(0, 'x'),\n",
    "                T.ones((1, width)))\n",
    "\n",
    "    x_t_flat = x_t.reshape((1, -1))\n",
    "    y_t_flat = y_t.reshape((1, -1))\n",
    "    ones = T.ones_like(x_t_flat)\n",
    "    grid = T.concatenate([x_t_flat, y_t_flat, ones], axis=0)\n",
    "    return grid\n",
    "    \n",
    "def _transform_affine(theta, input, downsample_factor):\n",
    "    num_batch, num_channels, height, width = input.shape\n",
    "    theta = T.reshape(theta, (-1, 2, 3))\n",
    "\n",
    "    # grid of (x_t, y_t, 1), eq (1) in ref [1]\n",
    "    out_height = T.cast(height // downsample_factor[0], 'int64')\n",
    "    out_width = T.cast(width // downsample_factor[1], 'int64')\n",
    "    grid = _meshgrid(out_height, out_width)\n",
    "\n",
    "    # Transform A x (x_t, y_t, 1)^T -> (x_s, y_s)\n",
    "    T_g = T.dot(theta, grid)\n",
    "    x_s = T_g[:, 0]\n",
    "    y_s = T_g[:, 1]\n",
    "    x_s_flat = x_s.flatten()\n",
    "    y_s_flat = y_s.flatten()\n",
    "\n",
    "    # dimshuffle input to  (bs, height, width, channels)\n",
    "    input_dim = input.dimshuffle(0, 2, 3, 1)\n",
    "    input_transformed = _interpolate(\n",
    "        input_dim, x_s_flat, y_s_flat,\n",
    "        out_height, out_width)\n",
    "\n",
    "    output = T.reshape(\n",
    "        input_transformed, (num_batch, out_height, out_width, num_channels))\n",
    "    output = output.dimshuffle(0, 3, 1, 2)  # dimshuffle to conv format\n",
    "    return output, T_g\n",
    "#OUTPUT: T_g is also included.\n",
    "\n",
    "def _linspace(start, stop, num):\n",
    "    # Theano linspace. Behaves similar to np.linspace\n",
    "    start = T.cast(start, theano.config.floatX)\n",
    "    stop = T.cast(stop, theano.config.floatX)\n",
    "    num = T.cast(num, theano.config.floatX)\n",
    "    step = (stop-start)/(num-1)\n",
    "    return T.arange(num, dtype=theano.config.floatX)*step+start\n",
    "\n",
    "def _interpolate(im, x, y, out_height, out_width):\n",
    "    # *_f are floats\n",
    "    num_batch, height, width, channels = im.shape\n",
    "    height_f = T.cast(height, theano.config.floatX)\n",
    "    width_f = T.cast(width, theano.config.floatX)\n",
    "\n",
    "    # clip coordinates to [-1, 1]\n",
    "    x = T.clip(x, -1, 1)\n",
    "    y = T.clip(y, -1, 1)\n",
    "\n",
    "    # scale coordinates from [-1, 1] to [0, width/height - 1]\n",
    "    x = (x + 1) / 2 * (width_f - 1)\n",
    "    y = (y + 1) / 2 * (height_f - 1)\n",
    "\n",
    "    # obtain indices of the 2x2 pixel neighborhood surrounding the coordinates;\n",
    "    # we need those in floatX for interpolation and in int64 for indexing. for\n",
    "    # indexing, we need to take care they do not extend past the image.\n",
    "    x0_f = T.floor(x)\n",
    "    y0_f = T.floor(y)\n",
    "    x1_f = x0_f + 1\n",
    "    y1_f = y0_f + 1\n",
    "    x0 = T.cast(x0_f, 'int64')\n",
    "    y0 = T.cast(y0_f, 'int64')\n",
    "    x1 = T.cast(T.minimum(x1_f, width_f - 1), 'int64')\n",
    "    y1 = T.cast(T.minimum(y1_f, height_f - 1), 'int64')\n",
    "\n",
    "    # The input is [num_batch, height, width, channels]. We do the lookup in\n",
    "    # the flattened input, i.e [num_batch*height*width, channels]. We need\n",
    "    # to offset all indices to match the flat version\n",
    "    dim2 = width\n",
    "    dim1 = width*height\n",
    "    base = T.repeat(\n",
    "        T.arange(num_batch, dtype='int64')*dim1, out_height*out_width)\n",
    "    base_y0 = base + y0*dim2\n",
    "    base_y1 = base + y1*dim2\n",
    "    idx_a = base_y0 + x0\n",
    "    idx_b = base_y1 + x0\n",
    "    idx_c = base_y0 + x1\n",
    "    idx_d = base_y1 + x1\n",
    "\n",
    "    # use indices to lookup pixels for all samples\n",
    "    im_flat = im.reshape((-1, channels))\n",
    "    Ia = im_flat[idx_a]\n",
    "    Ib = im_flat[idx_b]\n",
    "    Ic = im_flat[idx_c]\n",
    "    Id = im_flat[idx_d]\n",
    "\n",
    "    # calculate interpolated values\n",
    "    wa = ((x1_f-x) * (y1_f-y)).dimshuffle(0, 'x')\n",
    "    wb = ((x1_f-x) * (y-y0_f)).dimshuffle(0, 'x')\n",
    "    wc = ((x-x0_f) * (y1_f-y)).dimshuffle(0, 'x')\n",
    "    wd = ((x-x0_f) * (y-y0_f)).dimshuffle(0, 'x')\n",
    "    output = T.sum([wa*Ia, wb*Ib, wc*Ic, wd*Id], axis=0)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<theano.compile.function_module.Function object at 0x7f02b7778f90>\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor\n",
    "\n",
    "x = (_meshgrid(20, 20).shape)\n",
    "\n",
    "# concrete_x = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.int32)\n",
    "\n",
    "# x = theano.tensor.imatrix()\n",
    "# print x.shape.eval({x: concrete_x})\n",
    "print theano.function(inputs=[], outputs=x.shape)\n",
    "# print concrete_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: (100000, 1, 60, 60)\n",
      "Validation samples: (20000, 1, 60, 60)\n",
      "Test samples: (20000, 1, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# PARAMETERS\n",
    "t_start = time.time()\n",
    "test_dir = \"/home/dl2/caner_grad/MNIST_RESULTS/90_derece/10k_10k_test\"\n",
    "train_dir = \"/home/dl2/caner_grad/MNIST_RESULTS/90_derece/50k_50k_train\"\n",
    "out_file = \"mnist_cluttered_60x60_90degree_rotations.npz\"\n",
    "train_k = 5\n",
    "valid_k = 1\n",
    "n_classes = 10\n",
    "DIM = 60\n",
    "\n",
    "#COMMENTING STARTS IF THE FILE ALREADY EXISTS\n",
    "\n",
    "# test_file = open(test_dir, \"r\")\n",
    "# train_file = open(train_dir, \"r\")\n",
    "\n",
    "# test_file_lines = test_file.readlines()\n",
    "# test_images_loc = []\n",
    "# test_classes_pre = []\n",
    "# test_images_loc.append([test_file_lines[i].split(\" \")[0] for i in range(len(test_file_lines))])\n",
    "# test_classes_pre.append([test_file_lines[i].split(\" \")[1] for i in range(len(test_file_lines))])\n",
    "\n",
    "# trainandvalid_file_lines = train_file.readlines()\n",
    "# tv_size = len(trainandvalid_file_lines) \n",
    "# test_size = len(test_file_lines)\n",
    "\n",
    "# test_file.close()\n",
    "# train_file.close()\n",
    "\n",
    "# trainandvalid_images_loc = []\n",
    "# trainandvalid_classes_pre = []\n",
    "# trainandvalid_images_loc.append([trainandvalid_file_lines[i].split(\" \")[0] for i in range(tv_size)])\n",
    "# trainandvalid_classes_pre.append([trainandvalid_file_lines[i].split(\" \")[1] for i in range(tv_size)])\n",
    "\n",
    "# permuted_indexes = np.random.permutation(np.arange(len(trainandvalid_file_lines)))\n",
    "# train_indexes = permuted_indexes[:tv_size*train_k/(train_k+valid_k)]\n",
    "# valid_indexes = permuted_indexes[tv_size*train_k/(train_k+valid_k):tv_size]\n",
    "    \n",
    "# valid_classes_list = []\n",
    "# for i in xrange(tv_size*valid_k/(train_k+valid_k)):\n",
    "#     valid_classes_list.append(trainandvalid_classes_pre[0][valid_indexes[i]])\n",
    "\n",
    "# train_classes_list = []\n",
    "# for i in xrange(tv_size*train_k/(train_k+valid_k)):\n",
    "#     train_classes_list.append(trainandvalid_classes_pre[0][train_indexes[i]])\n",
    "    \n",
    "# test_classes = np.zeros((test_size, 10), dtype=np.int64)\n",
    "# for i in xrange(test_size):\n",
    "#     for j in range(n_classes):\n",
    "#         if int(test_classes_pre[0][i])==int(j):\n",
    "#             test_classes[i][j] = 1\n",
    "             \n",
    "# train_classes = np.zeros((tv_size*train_k/(valid_k+train_k), 10), dtype=np.int64)\n",
    "# for i in xrange(tv_size*train_k/(train_k+valid_k)):\n",
    "#     for j in xrange(n_classes):\n",
    "#         if(int(j)==int(train_classes_list[i])):\n",
    "#             train_classes[i][j] = 1 \n",
    "            \n",
    "# valid_classes = np.zeros((tv_size*valid_k/(valid_k+train_k), 10), dtype=np.int64)\n",
    "# for i in xrange(tv_size*valid_k/(train_k+valid_k)):\n",
    "#     for j in xrange(n_classes):\n",
    "#         if(int(j)==int(valid_classes_list[i])):\n",
    "#             valid_classes[i][j] = 1 \n",
    "\n",
    "# t_cl_end = time.time()\n",
    "# print t_cl_end - t_start\n",
    "\n",
    "# test_images = np.zeros((test_size, 1, DIM, DIM), dtype=np.float32)\n",
    "# train_images = np.zeros((tv_size*train_k/(valid_k+train_k), 1, DIM, DIM), dtype=np.float32)\n",
    "# valid_images = np.zeros((tv_size*valid_k/(valid_k+train_k), 1, DIM, DIM), dtype=np.float32)\n",
    "\n",
    "# for i in xrange(test_images.shape[0]):\n",
    "#     test_images[i] = cv2.imread(test_images_loc[0][i], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# for i in xrange(train_images.shape[0]):\n",
    "#     train_images[i] = cv2.imread(trainandvalid_images_loc[0][train_indexes[i]], cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "# for i in xrange(valid_images.shape[0]):\n",
    "#     valid_images[i] = cv2.imread(trainandvalid_images_loc[0][valid_indexes[i]], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# print np.mean(train_images[:], axis=0)\n",
    "# valid_images[:] = (valid_images - 128)/(128+0.0000001)\n",
    "# test_images[:] = (test_images - 128)/(128+0.0000001)\n",
    "# train_images[:] = (train_images - 128)/(128+0.0000001)\n",
    "\n",
    "# print np.mean(train_images[:], axis=0)\n",
    "# print np.max(test_images[0])\n",
    "# print np.mean(test_images[0])\n",
    "\n",
    "\n",
    "# dictionary = {'x_train':train_images, 'x_valid':valid_images, 'x_test':test_images, 'y_train':train_classes, \n",
    "#               'y_valid':valid_classes, 'y_test':test_classes}\n",
    "\n",
    "# t_dict_end = time.time()\n",
    "# print t_dict_end - t_cl_end\n",
    "\n",
    "# np.savez(out_file, x_train=dictionary['x_train'], y_train=dictionary['y_train'], x_test=dictionary['x_test'], y_test=dictionary['y_test'], x_valid=dictionary['x_valid'], y_valid=dictionary['y_valid']) \n",
    "# t_save_end = time.time()\n",
    "# print t_save_end - t_dict_end \n",
    "\n",
    "# COMMENTING END IF THE FILE ALREADY EXISTS\n",
    "\n",
    "mnist_90_rotated = out_file\n",
    "#THEN, BY USING A DIFFERENT NUMPY ARRAY DICTIONARY, WE WILL BE LOADING OUR DATA.\n",
    "def load_data(x):\n",
    "    data = np.load(x)\n",
    "    \n",
    "    X_train, y_train = data['x_train'], np.argmax(data['y_train'], axis=-1)\n",
    "    X_valid, y_valid = data['x_valid'], np.argmax(data['y_valid'], axis=-1)\n",
    "    X_test, y_test = data['x_test'], np.argmax(data['y_test'], axis=-1)\n",
    "\n",
    "    # reshape for convolutions\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, DIM, DIM))\n",
    "    X_valid = X_valid.reshape((X_valid.shape[0], 1, DIM, DIM))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, DIM, DIM))\n",
    "    \n",
    "    print \"Train samples:\", X_train.shape\n",
    "    print \"Validation samples:\", X_valid.shape\n",
    "    print \"Test samples:\", X_test.shape\n",
    "\n",
    "    return dict(\n",
    "        X_train=lasagne.utils.floatX(X_train),\n",
    "        y_train=y_train.astype('int32'),\n",
    "        X_valid=lasagne.utils.floatX(X_valid),\n",
    "        y_valid=y_valid.astype('int32'),\n",
    "        X_test=lasagne.utils.floatX(X_test),\n",
    "        y_test=y_test.astype('int32'),\n",
    "        num_examples_train=X_train.shape[0],\n",
    "        num_examples_valid=X_valid.shape[0],\n",
    "        num_examples_test=X_test.shape[0],\n",
    "        input_height=X_train.shape[2],\n",
    "        input_width=X_train.shape[3],\n",
    "        output_dim=10,)\n",
    "data = load_data(mnist_90_rotated)\n",
    "# print time.time() - t_save_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1. -1. -1. ..., -1. -1. -1.]\n",
      "  [-1. -1. -1. ..., -1. -1. -1.]\n",
      "  [-1. -1. -1. ..., -1. -1. -1.]\n",
      "  ..., \n",
      "  [-1. -1. -1. ..., -1. -1. -1.]\n",
      "  [-1. -1. -1. ..., -1. -1. -1.]\n",
      "  [-1. -1. -1. ..., -1. -1. -1.]]]\n",
      "[[[-1. -1. -1. ..., -1. -1. -1.]\n",
      "  [-1. -1. -1. ..., -1. -1. -1.]\n",
      "  [-1. -1. -1. ..., -1. -1. -1.]\n",
      "  ..., \n",
      "  [-1. -1. -1. ..., -1. -1. -1.]\n",
      "  [-1. -1. -1. ..., -1. -1. -1.]\n",
      "  [-1. -1. -1. ..., -1. -1. -1.]]]\n"
     ]
    }
   ],
   "source": [
    "print data['X_valid'][101]\n",
    "print data['X_train'][101]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAGwCAYAAADrFWH/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02f468bf90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(data['X_train'][101].reshape(DIM, DIM), cmap='gray', interpolation='none')\n",
    "plt.title('Cluttered MNIST', fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer network output shape:  (None, 1, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_width, input_height, output_dim,\n",
    "                batch_size=BATCH_SIZE):\n",
    "    ini = lasagne.init.HeUniform()\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 1, input_width, input_height),)\n",
    "\n",
    "    # Localization network\n",
    "    b = np.zeros((2, 3), dtype=theano.config.floatX)\n",
    "    b[0, 0] = 1\n",
    "    b[1, 1] = 1\n",
    "    b = b.flatten()\n",
    "    loc_l1 = pool(l_in, pool_size=(2, 2))\n",
    "    loc_l2 = conv(\n",
    "        loc_l1, num_filters=20, filter_size=(5, 5), W=ini)\n",
    "    loc_l3 = pool(loc_l2, pool_size=(2, 2))\n",
    "    loc_l4 = conv(loc_l3, num_filters=20, filter_size=(5, 5), W=ini)\n",
    "    loc_l5 = lasagne.layers.DenseLayer(\n",
    "        loc_l4, num_units=50, W=lasagne.init.HeUniform('relu'))\n",
    "    loc_out = lasagne.layers.DenseLayer(\n",
    "        loc_l5, num_units=6, b=b, W=lasagne.init.Constant(0.0), \n",
    "        nonlinearity=lasagne.nonlinearities.identity)\n",
    "    \n",
    "    # Transformer network\n",
    "    l_trans1 = TransformerLayer(l_in, loc_out, downsample_factor=1.0)\n",
    "    print \"Transformer network output shape: \", l_trans1.output_shape\n",
    "    \n",
    "    # Classification network\n",
    "    class_l1 = conv(\n",
    "        l_trans1,\n",
    "        num_filters=32,\n",
    "        filter_size=(3, 3),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=ini,\n",
    "    )\n",
    "    class_l2 = pool(class_l1, pool_size=(2, 2))\n",
    "    class_l3 = conv(\n",
    "        class_l2,\n",
    "        num_filters=32,\n",
    "        filter_size=(3, 3),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=ini,\n",
    "    )\n",
    "    class_l4 = pool(class_l3, pool_size=(2, 2))\n",
    "    class_l5 = lasagne.layers.DenseLayer(\n",
    "        class_l4,\n",
    "        num_units=256,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=ini,\n",
    "    )\n",
    "\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        class_l5,\n",
    "        num_units=output_dim,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax,\n",
    "        W=ini,\n",
    "    )\n",
    "\n",
    "    return l_out, l_trans1, loc_out\n",
    "\n",
    "model, l_transform, thetas = build_model(DIM, DIM, NUM_CLASSES)\n",
    "model_params = lasagne.layers.get_all_params(model, trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = T.tensor4()\n",
    "y = T.ivector()\n",
    "\n",
    "# training output\n",
    "output_train = lasagne.layers.get_output(model, X, deterministic=False)\n",
    "\n",
    "# evaluation output. Also includes output of transform for plotting\n",
    "output_eval, transform_eval, thetas_tensor = lasagne.layers.get_output([model, l_transform, thetas], X, deterministic=True)\n",
    "\n",
    "sh_lr = theano.shared(lasagne.utils.floatX(LEARNING_RATE))\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(output_train, y))\n",
    "cost_test = T.mean(T.nnet.categorical_crossentropy(output_eval, y))\n",
    "updates = lasagne.updates.adam(cost, model_params, learning_rate=sh_lr)\n",
    "\n",
    "train = theano.function([X, y], [cost, output_train], updates=updates)\n",
    "eval = theano.function([X, y], [output_eval, transform_eval, thetas_tensor, cost_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(X, y):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / float(BATCH_SIZE)))\n",
    "    loss = []\n",
    "    correct = 0\n",
    "    for i in range(num_batches):\n",
    "        idx = range(i*BATCH_SIZE, np.minimum((i+1)*BATCH_SIZE, num_samples))\n",
    "        X_batch = X[idx]\n",
    "        y_batch = y[idx]\n",
    "        cost_batch, output_train = train(X_batch, y_batch)\n",
    "        loss += [cost_batch]\n",
    "        preds = np.argmax(output_train, axis=-1)\n",
    "        correct += np.sum(y_batch == preds)\n",
    "\n",
    "    return np.mean(loss), correct / float(num_samples)\n",
    "\n",
    "\n",
    "def eval_epoch(X, y):\n",
    "    loss = []\n",
    "    output_eval, transform_eval, actual_thetas, loss = eval(X, y)\n",
    "    preds = np.argmax(output_eval, axis=-1)\n",
    "    acc = np.mean(preds == y)\n",
    "    return np.mean(loss), acc, transform_eval, actual_thetas, preds, output_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 2.57328510284, test acc 0.175\n"
     ]
    }
   ],
   "source": [
    "# BEFORE TRAINING, HOW IS THE ACCURACY LIKE?\n",
    "y_test2 = np.array([1, 1, 1, 1, 4, 4, 4, 4, 7, 0, 7, 2, 2, 2, 0, 0, 0, 2, 7, 7, 3, 3, 3, 3, 6, 6, 5, 5, 5, 5, 8, 6, 8, 6, 8, 8, 9, 9, 9, 9], dtype=np.int32)\n",
    "X_test2 = np.zeros((y_test2.shape[0], 1, 60, 60), dtype=np.float32)\n",
    "for i in range(40):\n",
    "    X_test2[i] = cv2.imread(\"/home/dl2/caner_grad/temp/translated_ones/\"+str(i)+ \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "for i in range(40):    \n",
    "    X_test2[i] = (X_test2[i] - np.mean(X_test2[i]))/np.std(X_test2[i])\n",
    "test_loss2, test_acc2, test_transform2, actual_thetas2, preds2, output_eval2 = eval_epoch(X_test2, y_test2)\n",
    "print \"test loss {0}, test acc {1}\".format(test_loss2, test_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(7,28))\n",
    "# k=20\n",
    "# for i in range(k):\n",
    "#     plt.subplot(k,2,1+i*2)\n",
    "#     plt.imshow(X_test2[i].reshape(DIM, DIM), cmap='gray', interpolation='none')\n",
    "#     if i == 0:\n",
    "#         plt.title('Original 60x60', fontsize=20)\n",
    "#     plt.axis('off')\n",
    "#     plt.subplot(k,2,2+i*2)\n",
    "#     plt.imshow(test_transform2[i].reshape(DIM//3, DIM//3), cmap='gray', interpolation='none')\n",
    "#     if i == 0:\n",
    "#         plt.title('Transformed 20x20', fontsize=20)\n",
    "#     plt.axis('off')\n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# SKIP IF ALREADY PRETRAINED\n",
    "theano.config.exception_verbosity=\"high\"\n",
    "\n",
    "valid_accs, train_accs, test_accs = [], [], []\n",
    "valid_losses, train_losses, test_losses = [], [], []\n",
    "try:\n",
    "    for n in range(201):\n",
    "        train_loss, train_acc = train_epoch(data['X_train'], data['y_train'])\n",
    "#         val_loss, valid_acc, valid_trainsform, dummy, preds_valid, output_eval_valid  = eval_epoch(data['X_valid'], data['y_valid'])\n",
    "#         valid_accs += [valid_acc]\n",
    "        train_accs += [train_acc]\n",
    "#         valid_losses += [val_loss]\n",
    "        train_losses += [train_loss]\n",
    "        \n",
    "        if n%10 == 0:\n",
    "            np.savez('model'+str(n)+'.npz', *lasagne.layers.get_all_param_values(model))\n",
    "\n",
    "        if (n+1) % 20 == 0:\n",
    "            new_lr = sh_lr.get_value() * 0.7\n",
    "            print \"New LR:\", new_lr\n",
    "            sh_lr.set_value(lasagne.utils.floatX(new_lr))\n",
    "\n",
    "#         print \"Epoch {0}: Train loss {1}, Train acc {2}, val acc {3}, test acc {4}\".format(\n",
    "#                 n, train_loss, train_acc, valid_acc, test_acc) \n",
    "#         print \"Epoch {0}: Train loss {1}, Train acc {2}, val acc {3}\".format(\n",
    "#                 n, train_loss, train_acc, valid_acc)\n",
    "        print \"Epoch {0}: Train loss {1}, Train acc {2}\".format(\n",
    "                n, train_loss, train_acc)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Error allocating 1081600000 bytes of device memory (out of memory).\nApply node that caused the error: GpuAllocEmpty(Assert{msg='The convolution would produce an invalid shape (dim[0] < 0).'}.0, Assert{msg='The convolution would produce an invalid shape (dim[1] < 0).'}.0, Assert{msg='The convolution would produce an invalid shape (dim[2] <= 0).'}.0, Assert{msg='The convolution would produce an invalid shape (dim[3] <= 0).'}.0)\nToposort index: 143\nInputs types: [TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]\nInputs shapes: [(), (), (), ()]\nInputs strides: [(), (), (), ()]\nInputs values: [array(20000), array(20), array(26), array(26)]\nInputs type_num: [7, 7, 7, 7]\nOutputs clients: [[GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConvDesc{border_mode='valid', subsample=(1, 1), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})]]\n\nDebugprint of the apply node: \nGpuAllocEmpty [id A] <CudaNdarrayType(float32, 4D)> ''   \n |Assert{msg='The convolution would produce an invalid shape (dim[0] < 0).'} [id B] <TensorType(int64, scalar)> ''   \n | |Shape_i{0} [id C] <TensorType(int64, scalar)> ''   \n | | |<TensorType(float32, 4D)> [id D] <TensorType(float32, 4D)>\n | |Elemwise{ge,no_inplace} [id E] <TensorType(bool, scalar)> ''   \n |   |Shape_i{0} [id C] <TensorType(int64, scalar)> ''   \n |   |TensorConstant{0} [id F] <TensorType(int8, scalar)>\n |Assert{msg='The convolution would produce an invalid shape (dim[1] < 0).'} [id G] <TensorType(int64, scalar)> ''   \n | |Shape_i{0} [id H] <TensorType(int64, scalar)> ''   \n | | |W [id I] <CudaNdarrayType(float32, (False, True, False, False))>\n | |Elemwise{ge,no_inplace} [id J] <TensorType(bool, scalar)> ''   \n |   |Shape_i{0} [id H] <TensorType(int64, scalar)> ''   \n |   |TensorConstant{0} [id F] <TensorType(int8, scalar)>\n |Assert{msg='The convolution would produce an invalid shape (dim[2] <= 0).'} [id K] <TensorType(int64, scalar)> ''   \n | |Elemwise{Composite{((i0 - (((i1 - i2) * i3) + i2)) + i2)}}[(0, 0)] [id L] <TensorType(int64, scalar)> ''   \n | | |Elemwise{Composite{maximum((i0 + ((i1 + i2) // i3)), i4)}} [id M] <TensorType(int64, scalar)> ''   \n | | | |TensorConstant{1} [id N] <TensorType(int64, scalar)>\n | | | |TensorConstant{-2} [id O] <TensorType(int64, scalar)>\n | | | |Shape_i{2} [id P] <TensorType(int64, scalar)> ''   \n | | | | |<TensorType(float32, 4D)> [id D] <TensorType(float32, 4D)>\n | | | |TensorConstant{2} [id Q] <TensorType(int64, scalar)>\n | | | |TensorConstant{0} [id R] <TensorType(int64, scalar)>\n | | |Shape_i{2} [id S] <TensorType(int64, scalar)> ''   \n | | | |W [id I] <CudaNdarrayType(float32, (False, True, False, False))>\n | | |TensorConstant{1} [id T] <TensorType(int8, scalar)>\n | | |TensorConstant{1} [id N] <TensorType(int64, scalar)>\n | |Elemwise{gt,no_inplace} [id U] <TensorType(bool, scalar)> ''   \n |   |Elemwise{Composite{((i0 - (((i1 - i2) * i3) + i2)) + i2)}}[(0, 0)] [id L] <TensorType(int64, scalar)> ''   \n |   |TensorConstant{0} [id F] <TensorType(int8, scalar)>\n |Assert{msg='The convolution would produce an invalid shape (dim[3] <= 0).'} [id V] <TensorType(int64, scalar)> ''   \n   |Elemwise{Composite{((i0 - (((i1 - i2) * i3) + i2)) + i2)}}[(0, 0)] [id W] <TensorType(int64, scalar)> ''   \n   | |Elemwise{Composite{maximum((i0 + ((i1 + i2) // i3)), i4)}} [id X] <TensorType(int64, scalar)> ''   \n   | | |TensorConstant{1} [id N] <TensorType(int64, scalar)>\n   | | |TensorConstant{-2} [id O] <TensorType(int64, scalar)>\n   | | |Shape_i{3} [id Y] <TensorType(int64, scalar)> ''   \n   | | | |<TensorType(float32, 4D)> [id D] <TensorType(float32, 4D)>\n   | | |TensorConstant{2} [id Q] <TensorType(int64, scalar)>\n   | | |TensorConstant{0} [id R] <TensorType(int64, scalar)>\n   | |Shape_i{3} [id Z] <TensorType(int64, scalar)> ''   \n   | | |W [id I] <CudaNdarrayType(float32, (False, True, False, False))>\n   | |TensorConstant{1} [id T] <TensorType(int8, scalar)>\n   | |TensorConstant{1} [id N] <TensorType(int64, scalar)>\n   |Elemwise{gt,no_inplace} [id BA] <TensorType(bool, scalar)> ''   \n     |Elemwise{Composite{((i0 - (((i1 - i2) * i3) + i2)) + i2)}}[(0, 0)] [id W] <TensorType(int64, scalar)> ''   \n     |TensorConstant{0} [id F] <TensorType(int8, scalar)>\n\nStorage map footprint:\n - Alloc.0, Shape: (20000, 3600), ElemSize: 8 Byte(s), TotalSize: 576000000 Byte(s)\n - GpuElemwise{Sub}[(0, 1)].0, Shape: (72000000, 1), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuReshape{2}.0, Shape: (72000000, 1), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuElemwise{Add}[(0, 1)].0, Shape: (72000000,), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - <TensorType(float32, 4D)>, Input, Shape: (20000, 1, 60, 60), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuElemwise{mul,no_inplace}.0, Shape: (72000000, 1), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuElemwise{Sub}[(0, 1)].0, Shape: (72000000, 1), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuFromHost.0, Shape: (20000, 1, 60, 60), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuElemwise{sub,no_inplace}.0, Shape: (72000000, 1), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuElemwise{Add}[(0, 1)].0, Shape: (72000000,), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - HostFromGpu.0, Shape: (72000000,), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - HostFromGpu.0, Shape: (72000000,), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuElemwise{sub,no_inplace}.0, Shape: (72000000, 1), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuContiguous.0, Shape: (20000, 1, 30, 30), ElemSize: 4 Byte(s), TotalSize: 72000000 Byte(s)\n - W, Shared Input, Shape: (5408, 256), ElemSize: 4 Byte(s), TotalSize: 5537792 Byte(s)\n - GpuElemwise{Add}[(0, 0)].0, Shape: (20000, 6), ElemSize: 4 Byte(s), TotalSize: 480000 Byte(s)\n - W, Shared Input, Shape: (1620, 50), ElemSize: 4 Byte(s), TotalSize: 324000 Byte(s)\n - <TensorType(int32, vector)>, Input, Shape: (20000,), ElemSize: 4 Byte(s), TotalSize: 80000 Byte(s)\n - W, Shared Input, Shape: (20, 20, 5, 5), ElemSize: 4 Byte(s), TotalSize: 40000 Byte(s)\n - W, Shared Input, Shape: (32, 32, 3, 3), ElemSize: 4 Byte(s), TotalSize: 36864 Byte(s)\n - W, Shared Input, Shape: (256, 10), ElemSize: 4 Byte(s), TotalSize: 10240 Byte(s)\n - W, Shared Input, Shape: (20, 1, 5, 5), ElemSize: 4 Byte(s), TotalSize: 2000 Byte(s)\n - GpuContiguous.0, Shape: (20, 1, 5, 5), ElemSize: 4 Byte(s), TotalSize: 2000 Byte(s)\n - W, Shared Input, Shape: (50, 6), ElemSize: 4 Byte(s), TotalSize: 1200 Byte(s)\n - W, Shared Input, Shape: (32, 1, 3, 3), ElemSize: 4 Byte(s), TotalSize: 1152 Byte(s)\n - b, Shared Input, Shape: (256,), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - b, Shared Input, Shape: (50,), ElemSize: 4 Byte(s), TotalSize: 200 Byte(s)\n - b, Shared Input, Shape: (32,), ElemSize: 4 Byte(s), TotalSize: 128 Byte(s)\n - b, Shared Input, Shape: (32,), ElemSize: 4 Byte(s), TotalSize: 128 Byte(s)\n - b, Shared Input, Shape: (20,), ElemSize: 4 Byte(s), TotalSize: 80 Byte(s)\n - b, Shared Input, Shape: (20,), ElemSize: 4 Byte(s), TotalSize: 80 Byte(s)\n - b, Shared Input, Shape: (10,), ElemSize: 4 Byte(s), TotalSize: 40 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (4,), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (4,), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (4,), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (4,), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - b, Shared Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)\n - TensorConstant{(2,) of 2}, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - TensorConstant{(2,) of 0}, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - TensorConstant{-2}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{(1,) of -1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{((i0 * i1 * i2) // (-(i3 * i4 * i5 * i2)))}}[(0, 1)].0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Assert{msg='The convolution would produce an invalid shape (dim[1] < 0).'}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{2}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{3}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{6}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{13}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Assert{msg='The convolution would produce an invalid shape (dim[2] <= 0).'}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{-1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{2}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{3}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{20}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Assert{msg='The convolution would produce an invalid shape (dim[3] <= 0).'}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Assert{msg='The convolution would produce an invalid shape (dim[2] <= 0).'}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Assert{msg='The convolution would produce an invalid shape (dim[3] <= 0).'}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Mul}[(0, 0)].0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Assert{msg='The convolution would produce an invalid shape (dim[0] < 0).'}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{2}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{29}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{3}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Cast{int64}(Cast{float64}(i0))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Assert{msg='The convolution would produce an invalid shape (dim[1] < 0).'}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{32}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - InplaceDimShuffle{x}.0, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8 Byte(s)\n - TensorConstant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - GpuElemwise{Add}[(0, 1)].0, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[ 0.5]}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{(1,) of -1.0}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{0.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - Elemwise{Cast{float32}}.0, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{[ 1.]}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{(1, 1) of 1.0}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[[[ 0.5]]]]}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - GpuElemwise{Add}[(0, 1)].0, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 0.5]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{(1,) of 2.0}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{1.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{[-1.]}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - Constant{1.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - TensorConstant{-1}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{20}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{60}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{3}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - Elemwise{eq,no_inplace}.0, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - Elemwise{eq,no_inplace}.0, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{0}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{32}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - Elemwise{eq,no_inplace}.0, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{1}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{5}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{30}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n TotalSize: 4110517456.0 Byte(s) 3.828 GB\n TotalSize inputs: 294035153.0 Byte(s) 0.274 GB\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ae086389a21e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m      \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_all_param_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_thetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_accs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8ca706570cd1>\u001b[0m in \u001b[0;36meval_epoch\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moutput_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_thetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    899\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Error allocating 1081600000 bytes of device memory (out of memory).\nApply node that caused the error: GpuAllocEmpty(Assert{msg='The convolution would produce an invalid shape (dim[0] < 0).'}.0, Assert{msg='The convolution would produce an invalid shape (dim[1] < 0).'}.0, Assert{msg='The convolution would produce an invalid shape (dim[2] <= 0).'}.0, Assert{msg='The convolution would produce an invalid shape (dim[3] <= 0).'}.0)\nToposort index: 143\nInputs types: [TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]\nInputs shapes: [(), (), (), ()]\nInputs strides: [(), (), (), ()]\nInputs values: [array(20000), array(20), array(26), array(26)]\nInputs type_num: [7, 7, 7, 7]\nOutputs clients: [[GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConvDesc{border_mode='valid', subsample=(1, 1), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})]]\n\nDebugprint of the apply node: \nGpuAllocEmpty [id A] <CudaNdarrayType(float32, 4D)> ''   \n |Assert{msg='The convolution would produce an invalid shape (dim[0] < 0).'} [id B] <TensorType(int64, scalar)> ''   \n | |Shape_i{0} [id C] <TensorType(int64, scalar)> ''   \n | | |<TensorType(float32, 4D)> [id D] <TensorType(float32, 4D)>\n | |Elemwise{ge,no_inplace} [id E] <TensorType(bool, scalar)> ''   \n |   |Shape_i{0} [id C] <TensorType(int64, scalar)> ''   \n |   |TensorConstant{0} [id F] <TensorType(int8, scalar)>\n |Assert{msg='The convolution would produce an invalid shape (dim[1] < 0).'} [id G] <TensorType(int64, scalar)> ''   \n | |Shape_i{0} [id H] <TensorType(int64, scalar)> ''   \n | | |W [id I] <CudaNdarrayType(float32, (False, True, False, False))>\n | |Elemwise{ge,no_inplace} [id J] <TensorType(bool, scalar)> ''   \n |   |Shape_i{0} [id H] <TensorType(int64, scalar)> ''   \n |   |TensorConstant{0} [id F] <TensorType(int8, scalar)>\n |Assert{msg='The convolution would produce an invalid shape (dim[2] <= 0).'} [id K] <TensorType(int64, scalar)> ''   \n | |Elemwise{Composite{((i0 - (((i1 - i2) * i3) + i2)) + i2)}}[(0, 0)] [id L] <TensorType(int64, scalar)> ''   \n | | |Elemwise{Composite{maximum((i0 + ((i1 + i2) // i3)), i4)}} [id M] <TensorType(int64, scalar)> ''   \n | | | |TensorConstant{1} [id N] <TensorType(int64, scalar)>\n | | | |TensorConstant{-2} [id O] <TensorType(int64, scalar)>\n | | | |Shape_i{2} [id P] <TensorType(int64, scalar)> ''   \n | | | | |<TensorType(float32, 4D)> [id D] <TensorType(float32, 4D)>\n | | | |TensorConstant{2} [id Q] <TensorType(int64, scalar)>\n | | | |TensorConstant{0} [id R] <TensorType(int64, scalar)>\n | | |Shape_i{2} [id S] <TensorType(int64, scalar)> ''   \n | | | |W [id I] <CudaNdarrayType(float32, (False, True, False, False))>\n | | |TensorConstant{1} [id T] <TensorType(int8, scalar)>\n | | |TensorConstant{1} [id N] <TensorType(int64, scalar)>\n | |Elemwise{gt,no_inplace} [id U] <TensorType(bool, scalar)> ''   \n |   |Elemwise{Composite{((i0 - (((i1 - i2) * i3) + i2)) + i2)}}[(0, 0)] [id L] <TensorType(int64, scalar)> ''   \n |   |TensorConstant{0} [id F] <TensorType(int8, scalar)>\n |Assert{msg='The convolution would produce an invalid shape (dim[3] <= 0).'} [id V] <TensorType(int64, scalar)> ''   \n   |Elemwise{Composite{((i0 - (((i1 - i2) * i3) + i2)) + i2)}}[(0, 0)] [id W] <TensorType(int64, scalar)> ''   \n   | |Elemwise{Composite{maximum((i0 + ((i1 + i2) // i3)), i4)}} [id X] <TensorType(int64, scalar)> ''   \n   | | |TensorConstant{1} [id N] <TensorType(int64, scalar)>\n   | | |TensorConstant{-2} [id O] <TensorType(int64, scalar)>\n   | | |Shape_i{3} [id Y] <TensorType(int64, scalar)> ''   \n   | | | |<TensorType(float32, 4D)> [id D] <TensorType(float32, 4D)>\n   | | |TensorConstant{2} [id Q] <TensorType(int64, scalar)>\n   | | |TensorConstant{0} [id R] <TensorType(int64, scalar)>\n   | |Shape_i{3} [id Z] <TensorType(int64, scalar)> ''   \n   | | |W [id I] <CudaNdarrayType(float32, (False, True, False, False))>\n   | |TensorConstant{1} [id T] <TensorType(int8, scalar)>\n   | |TensorConstant{1} [id N] <TensorType(int64, scalar)>\n   |Elemwise{gt,no_inplace} [id BA] <TensorType(bool, scalar)> ''   \n     |Elemwise{Composite{((i0 - (((i1 - i2) * i3) + i2)) + i2)}}[(0, 0)] [id W] <TensorType(int64, scalar)> ''   \n     |TensorConstant{0} [id F] <TensorType(int8, scalar)>\n\nStorage map footprint:\n - Alloc.0, Shape: (20000, 3600), ElemSize: 8 Byte(s), TotalSize: 576000000 Byte(s)\n - GpuElemwise{Sub}[(0, 1)].0, Shape: (72000000, 1), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuReshape{2}.0, Shape: (72000000, 1), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuElemwise{Add}[(0, 1)].0, Shape: (72000000,), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - <TensorType(float32, 4D)>, Input, Shape: (20000, 1, 60, 60), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuElemwise{mul,no_inplace}.0, Shape: (72000000, 1), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuElemwise{Sub}[(0, 1)].0, Shape: (72000000, 1), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuFromHost.0, Shape: (20000, 1, 60, 60), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuElemwise{sub,no_inplace}.0, Shape: (72000000, 1), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuElemwise{Add}[(0, 1)].0, Shape: (72000000,), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - HostFromGpu.0, Shape: (72000000,), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - HostFromGpu.0, Shape: (72000000,), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuElemwise{sub,no_inplace}.0, Shape: (72000000, 1), ElemSize: 4 Byte(s), TotalSize: 288000000 Byte(s)\n - GpuContiguous.0, Shape: (20000, 1, 30, 30), ElemSize: 4 Byte(s), TotalSize: 72000000 Byte(s)\n - W, Shared Input, Shape: (5408, 256), ElemSize: 4 Byte(s), TotalSize: 5537792 Byte(s)\n - GpuElemwise{Add}[(0, 0)].0, Shape: (20000, 6), ElemSize: 4 Byte(s), TotalSize: 480000 Byte(s)\n - W, Shared Input, Shape: (1620, 50), ElemSize: 4 Byte(s), TotalSize: 324000 Byte(s)\n - <TensorType(int32, vector)>, Input, Shape: (20000,), ElemSize: 4 Byte(s), TotalSize: 80000 Byte(s)\n - W, Shared Input, Shape: (20, 20, 5, 5), ElemSize: 4 Byte(s), TotalSize: 40000 Byte(s)\n - W, Shared Input, Shape: (32, 32, 3, 3), ElemSize: 4 Byte(s), TotalSize: 36864 Byte(s)\n - W, Shared Input, Shape: (256, 10), ElemSize: 4 Byte(s), TotalSize: 10240 Byte(s)\n - W, Shared Input, Shape: (20, 1, 5, 5), ElemSize: 4 Byte(s), TotalSize: 2000 Byte(s)\n - GpuContiguous.0, Shape: (20, 1, 5, 5), ElemSize: 4 Byte(s), TotalSize: 2000 Byte(s)\n - W, Shared Input, Shape: (50, 6), ElemSize: 4 Byte(s), TotalSize: 1200 Byte(s)\n - W, Shared Input, Shape: (32, 1, 3, 3), ElemSize: 4 Byte(s), TotalSize: 1152 Byte(s)\n - b, Shared Input, Shape: (256,), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - b, Shared Input, Shape: (50,), ElemSize: 4 Byte(s), TotalSize: 200 Byte(s)\n - b, Shared Input, Shape: (32,), ElemSize: 4 Byte(s), TotalSize: 128 Byte(s)\n - b, Shared Input, Shape: (32,), ElemSize: 4 Byte(s), TotalSize: 128 Byte(s)\n - b, Shared Input, Shape: (20,), ElemSize: 4 Byte(s), TotalSize: 80 Byte(s)\n - b, Shared Input, Shape: (20,), ElemSize: 4 Byte(s), TotalSize: 80 Byte(s)\n - b, Shared Input, Shape: (10,), ElemSize: 4 Byte(s), TotalSize: 40 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (4,), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (4,), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (4,), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (4,), ElemSize: 8 Byte(s), TotalSize: 32 Byte(s)\n - b, Shared Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)\n - TensorConstant{(2,) of 2}, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - MakeVector{dtype='int64'}.0, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - TensorConstant{(2,) of 0}, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - TensorConstant{-2}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{(1,) of -1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{((i0 * i1 * i2) // (-(i3 * i4 * i5 * i2)))}}[(0, 1)].0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Assert{msg='The convolution would produce an invalid shape (dim[1] < 0).'}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{2}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{3}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{6}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{13}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Assert{msg='The convolution would produce an invalid shape (dim[2] <= 0).'}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{-1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{2}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{3}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{20}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Assert{msg='The convolution would produce an invalid shape (dim[3] <= 0).'}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Assert{msg='The convolution would produce an invalid shape (dim[2] <= 0).'}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Assert{msg='The convolution would produce an invalid shape (dim[3] <= 0).'}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Mul}[(0, 0)].0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Assert{msg='The convolution would produce an invalid shape (dim[0] < 0).'}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{2}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{29}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Switch(EQ(i0, i1), i2, i0)}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{3}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{Cast{int64}(Cast{float64}(i0))}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Assert{msg='The convolution would produce an invalid shape (dim[1] < 0).'}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{32}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{1}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - InplaceDimShuffle{x}.0, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8 Byte(s)\n - TensorConstant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Constant{0.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - GpuElemwise{Add}[(0, 1)].0, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[ 0.5]}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{(1,) of -1.0}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{0.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - Elemwise{Cast{float32}}.0, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{[ 1.]}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{(1, 1) of 1.0}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[[[ 0.5]]]]}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - GpuElemwise{Add}[(0, 1)].0, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 0.5]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{(1,) of 2.0}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{1.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{[-1.]}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - Constant{1.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - TensorConstant{-1}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{20}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{60}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{3}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - Elemwise{eq,no_inplace}.0, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - Elemwise{eq,no_inplace}.0, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{0}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{32}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - Elemwise{eq,no_inplace}.0, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{1}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{5}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n - TensorConstant{30}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n TotalSize: 4110517456.0 Byte(s) 3.828 GB\n TotalSize inputs: 294035153.0 Byte(s) 0.274 GB\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'."
     ]
    }
   ],
   "source": [
    "theano.config.exception_verbosity=\"high\"\n",
    "with np.load('model190.npz') as f:\n",
    "     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(model, param_values)\n",
    "test_loss, test_acc, test_transform, actual_thetas, preds, output_eval = eval_epoch(data['X_test'], data['y_test'])\n",
    "test_accs += [test_acc]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print valid_losses\n",
    "print test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.title('Accuracies', fontsize=20)\n",
    "plt.plot(1-np.array(train_accs), label='Training Error')\n",
    "plt.plot(1-np.array(valid_accs), label='Validation Error')\n",
    "plt.plot(1-np.array(test_accs), label='Testing Error')\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=20)\n",
    "plt.ylabel('Error', fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(range(len(train_losses)), train_losses, label='Training Loss')\n",
    "plt.plot(range(len(valid_losses)), valid_losses, label='Validation Loss')\n",
    "plt.plot(range(len(test_losses)), test_losses, label='Testing Loss')\n",
    "plt.xlabel('Epoch', fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.title('Train Loss', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print data['X_train'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,14))\n",
    "k=7\n",
    "for i in [2000,2001,2002,2003,2004,2005,2006]:\n",
    "    plt.subplot(k,2,1+(i-2000)*2)\n",
    "    plt.imshow(data['X_test'][i].reshape(DIM, DIM), cmap='gray', interpolation='none')\n",
    "    if i == 0:\n",
    "        plt.title('Original 60x60', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(k,2,2+(i-2000)*2)\n",
    "    plt.imshow(test_transform[i].reshape(DIM//3, DIM//3), cmap='gray', interpolation='none')\n",
    "    if i == 0:\n",
    "        plt.title('Transformed 20x20', fontsize=20)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "r_theta = (lambda theta: np.array([[1, 0, 0 ], [0, np.cos(theta), -np.sin(theta)], [0, np.sin(theta), np.cos(theta)]]))\n",
    "r_fi = (lambda fi: np.array([[np.cos(fi), 0, np.sin(fi)], [0, 1, 0], [-np.sin(fi), 0, np.cos(fi)]]))\n",
    "r_phi = (lambda phi: np.array([[np.cos(phi), -np.sin(phi), 0], [np.sin(phi), np.cos(phi), 0], [0, 0, 1]]))\n",
    "\n",
    "#print r_phi(50)*r_fi(45)*r_theta(15)\n",
    "klm = lambda theta, fi, phi:np.array([[1, 0, 0 ], [0, np.cos(theta), -np.sin(theta)], [0, np.sin(theta), np.cos(theta)]])*np.array([[np.cos(fi), 0, np.sin(fi)], [0, 1, 0], [-np.sin(fi), 0, np.cos(fi)]])*np.array([[np.cos(phi), -np.sin(phi), 0], [np.sin(phi), np.cos(phi), 0], [0, 0, 1]])\n",
    "print klm(0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def matrix_to_params(inputs):\n",
    "    translation_x, translation_y = inputs[2], inputs[5]\n",
    "    temp_matrix = [[inputs[0], inputs[1]], [inputs[3], inputs[4]]]\n",
    "    angle = np.arctan(temp_matrix[1][0]/temp_matrix[0][0])\n",
    "    angle2 = np.arctan(-temp_matrix[0][1]/temp_matrix[1][1])\n",
    "    scale_x = inputs[0]/np.cos(angle)\n",
    "    scale_y = inputs[4]/np.cos(angle)\n",
    "    return scale_x, scale_y, angle*180, angle2*180, translation_x, translation_y\n",
    "print actual_thetas[:5] \n",
    "for i in range(5):\n",
    "    print matrix_to_params(actual_thetas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_test3 = np.array([1, 1, 1, 1, 4, 4, 4, 4, 7, 0, 7, 2, 2, 2, 0, 0, 0, 2, 7, 7, 3, 3, 3, 3, 6, 6, 5, 5, 5, 5, 8, 6, 8, 6, 8, 8, 9, 9, 9, 9], dtype=np.int32)\n",
    "X_test3 = np.zeros((y_test3.shape[0], 1, 60, 60), dtype=np.float32)\n",
    "print y_test3.shape\n",
    "for i in range(40):\n",
    "    X_test3[i] = (cv2.imread(\"/home/dl2/caner_grad/temp/translated_ones/\"+str(i)+ \".png\", cv2.IMREAD_GRAYSCALE))\n",
    "mean_target = np.mean(data[\"X_train\"], axis=0)\n",
    "std_target = np.std(data[\"X_train\"], axis=0)\n",
    "mean_test = (np.mean(X_test3, axis=0))\n",
    "std_test = (np.std(X_test3, axis=0))+1e-1/5\n",
    "\n",
    "X_test3_new = (X_test3 - mean_test + mean_target)/std_test*std_target\n",
    "print X_test3_new[0]\n",
    "# print(np.std(X_test3, axis=0))\n",
    "\n",
    "\n",
    "test_loss3, test_acc3, test_transform3, actual_thetas3, preds3, output_eval3 = eval_epoch(X_test3_new, y_test3)\n",
    "print \"test loss {0}, test acc {1}\".format(test_loss3, test_acc3)\n",
    "print(np.mean(X_test3_new[0]), np.std(X_test3_new[0]))\n",
    "print(np.mean(data['X_test']), np.std(data['X_test']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "M = cv2.getRotationMatrix2D((30, 30),90,1)\n",
    "\n",
    "k=7\n",
    "\n",
    "y_test3 = data[\"y_test\"][:k].copy()\n",
    "X_test3 = data[\"X_test\"][:k].copy()\n",
    "print(X_test3.shape)\n",
    "\n",
    "for i in range(k):\n",
    "    a = cv2.warpAffine(np.squeeze(X_test3[i]),M,(60,60))\n",
    "    X_test3[i] = np.expand_dims(a, 0)\n",
    "    \n",
    "\n",
    "test_loss3, test_acc3, test_transform3, actual_thetas3, preds_rot, output_eval3 = eval_epoch(X_test3, y_test3)\n",
    "print \"test loss {0}, test acc {1}\".format(test_loss3, test_acc3)\n",
    "print(np.mean(X_test3[0]), np.std(X_test3[0]))\n",
    "print(np.mean(data['X_test']), np.std(data['X_test']))\n",
    "print preds_rot\n",
    "print actual_thetas3[:8] \n",
    "print output_eval3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,14))\n",
    "k=7\n",
    "for i in range(k):\n",
    "    \n",
    "    plt.subplot(k,2,1+i*2)\n",
    "    plt.imshow(X_test3[i].reshape(DIM, DIM), cmap='gray', interpolation='none')\n",
    "    if i == 0:\n",
    "        plt.title('Original 60x60', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(k,2,2+i*2)\n",
    "    plt.imshow(test_transform3[i].reshape(DIM//3, DIM//3), cmap='gray', interpolation='none')\n",
    "    if i == 0:\n",
    "        plt.title('Transformed 20x20', fontsize=20)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap              )\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=60)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float16') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, np.around(cm[i, j],decimals = 3),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "y_true = data['y_test']\n",
    "y_pred = np.argmax(output_eval, axis=1)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "class_names = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,7))\n",
    "\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
